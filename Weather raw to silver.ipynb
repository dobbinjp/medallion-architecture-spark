{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rawfolder = \"abfss://raw@dobbinsdatalake.dfs.core.windows.net/weatherbit/*.json\"\r\n",
        "schema_file = \"abfss://raw@dobbinsdatalake.dfs.core.windows.net/weatherbit/2023_07_21_16_00_23_UTC.json\"\r\n",
        "silver_folder = \"abfss://silver@dobbinsdatalake.dfs.core.windows.net/delta/weather_silver/weather\""
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_schema = spark.read.format(\"json\").option(\"inferSchema\", True).load(schema_file).schema"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.readStream.format(\"json\").schema(df_schema).load(rawfolder).withColumn(\"SourceFileName\", F.input_file_name())"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (\r\n",
        "    df.withColumn('exploded', F.explode('data').alias('exploded'))\r\n",
        "    .withColumn('pres', F.expr('exploded.pres'))\r\n",
        "    .withColumn('snow', F.expr('exploded.snow'))\r\n",
        "    .withColumn('uv', F.expr('exploded.uv'))\r\n",
        "    .withColumn('aqi', F.expr('exploded.aqi'))\r\n",
        "    .withColumn('gust', F.expr('exploded.gust'))\r\n",
        "    .withColumn('precip', F.expr('exploded.precip'))\r\n",
        "    .withColumn('rh', F.expr('exploded.rh'))\r\n",
        "    .withColumn('app_temp', F.expr('exploded.app_temp'))\r\n",
        "    .withColumn('timezone', F.expr('exploded.timezone'))\r\n",
        "    .withColumn('wind_spd', F.expr('exploded.wind_spd'))\r\n",
        "    .withColumn('ts', F.expr('exploded.ts'))\r\n",
        "    .withColumn('datetime', F.expr('exploded.datetime'))\r\n",
        "    .withColumn('vis', F.expr('exploded.vis'))\r\n",
        "    .withColumn('solar_rad', F.expr('exploded.solar_rad'))\r\n",
        "    .withColumn('pod', F.expr('exploded.pod'))\r\n",
        "    .withColumn('country_code', F.expr('exploded.country_code'))\r\n",
        "    .withColumn('clouds', F.expr('exploded.clouds'))\r\n",
        "    .withColumn('elev_angle', F.expr('exploded.elev_angle'))\r\n",
        "    .withColumn('dni', F.expr('exploded.dni'))\r\n",
        "    .withColumn('temp', F.expr('exploded.temp'))\r\n",
        "    .withColumn('ob_time', F.expr('exploded.ob_time'))\r\n",
        "    .withColumn('sunrise', F.expr('exploded.sunrise'))\r\n",
        "    .withColumn('wind_cdir_full', F.expr('exploded.wind_cdir_full'))\r\n",
        "    .withColumn('state_code', F.expr('exploded.state_code'))\r\n",
        "    .withColumn('lon', F.expr('exploded.lon'))\r\n",
        "    .withColumn('city_name', F.expr('exploded.city_name'))\r\n",
        "    .withColumn('sunset', F.expr('exploded.sunset'))\r\n",
        "    .withColumn('slp', F.expr('exploded.slp'))\r\n",
        "    .withColumn('weather_code', F.expr('exploded.weather.code'))\r\n",
        "    .withColumn('weather_description', F.expr('exploded.weather.description'))\r\n",
        "    .withColumn('weather_icon', F.expr('exploded.weather.icon'))\r\n",
        "    .withColumn('h_angle', F.expr('exploded.h_angle'))\r\n",
        "    .withColumn('ghi', F.expr('exploded.ghi'))\r\n",
        "    .withColumn('wind_dir', F.expr('exploded.wind_dir'))\r\n",
        "    .withColumn('dewpt', F.expr('exploded.dewpt'))\r\n",
        "    .withColumn('dhi', F.expr('exploded.dhi'))\r\n",
        "    .withColumn('lat', F.expr('exploded.lat'))\r\n",
        "    .withColumn('wind_cdir', F.expr('exploded.wind_cdir'))\r\n",
        "    .withColumn('station', F.expr('exploded.station'))\r\n",
        "    .withColumn('SourceFileName', F.regexp_replace(\"SourceFileName\", \"abfss://raw@dobbinsdatalake.dfs.core.windows.net/weatherbit/\", \"\"))\r\n",
        "    .drop(*('data','exploded','count'))\r\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # this works and is much simpler, leave it in for demo purposes\r\n",
        "\r\n",
        "# display(spark.sql(f\"\"\" \r\n",
        "#     with cte as \r\n",
        "#     (\r\n",
        "#     select explode(data) as exploded, * \r\n",
        "#     from df\r\n",
        "#     )\r\n",
        "#     select exploded.weather.*, exploded.* from cte\r\n",
        "# \"\"\"))\r\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamingQuery = (df\r\n",
        "    .writeStream\r\n",
        "    .format(\"delta\")\r\n",
        "    .option(\"checkpointLocation\", silver_folder + \"/checkpoint\")\r\n",
        "    .option(\"mergeSchema\", True)\r\n",
        "    .outputMode(\"append\")\r\n",
        "    .queryName(\"weather_stream\")\r\n",
        "    .trigger(once=True)\r\n",
        "    .start(silver_folder)\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for s in spark.streams.active:\r\n",
        "    print(s.name)\r\n",
        "    print(s.status)\r\n",
        "    print(s.lastProgress)\r\n",
        "    s.awaitTermination()"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "create database if not exists weather_silver;"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "use database weather_silver;"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "create table if not exists weather_silver.weather\r\n",
        "using delta\r\n",
        "location 'abfss://silver@dobbinsdatalake.dfs.core.windows.net/delta/weather_silver/weather'"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time travel and change data feed\r\n",
        "###### This is for \"clone and demo purposes,\" do not leave uncommented"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%sql\r\n",
        "\r\n",
        "#-- ALTER TABLE weather_silver.weather SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%sql\r\n",
        "\r\n",
        "# DESCRIBE extended weather_silver.weather\r\n",
        "# DESCRIBE history weather_silver.weather\r\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version as ints or longs\r\n",
        "\r\n",
        "# display(spark.read.format(\"delta\") \\\r\n",
        "#   .option(\"readChangeFeed\", \"true\") \\\r\n",
        "#   .option(\"startingVersion\", 24) \\\r\n",
        "#   .option(\"endingVersion\", 25) \\\r\n",
        "#   .table(\"weather_silver.weather\"))"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df4 = spark.read\\\r\n",
        "#   .format('delta')\\\r\n",
        "#   .option('versionAsOf', 20)\\\r\n",
        "#   .load(silver_folder)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this will run the silver to gold notebook using the same spark session\r\n",
        "\r\n",
        "# mssparkutils.notebook.run(\"Weather silver to gold\", 90)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mssparkutils.notebook.exit('Success')"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}